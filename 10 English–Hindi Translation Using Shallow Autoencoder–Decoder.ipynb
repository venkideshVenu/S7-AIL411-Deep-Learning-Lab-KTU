{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c39143d-d8a8-45c7-a9bf-27cbc5168672",
   "metadata": {},
   "source": [
    "# Experiment : 10\n",
    "##   Implement a shallow auto encoder and decoder network for machine translation(by using Kaggle English to Hindi neural translation dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48d3d92-2de8-496e-8fbf-ee5f1accbd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "\n",
      "======================================================================\n",
      "LOADING KAGGLE ENGLISH-HINDI TRANSLATION DATASET\n",
      "======================================================================\n",
      "Dataset loaded with 30 sentence pairs\n",
      "\n",
      "Sample data:\n",
      "  english_sentence hindi_sentence\n",
      "0            hello         नमस्ते\n",
      "1      how are you    आप कैसे हैं\n",
      "2     good morning       सुप्रभात\n",
      "3        thank you        धन्यवाद\n",
      "4          goodbye         अलविदा\n",
      "\n",
      "======================================================================\n",
      "PREPROCESSING DATA\n",
      "======================================================================\n",
      "Total sentences: 30\n",
      "\n",
      "======================================================================\n",
      "TOKENIZATION AND VOCABULARY BUILDING\n",
      "======================================================================\n",
      "English vocabulary size: 55\n",
      "Hindi vocabulary size: 61\n",
      "English padded shape: (30, 15)\n",
      "Hindi padded shape: (30, 15)\n",
      "Decoder input shape: (30, 14)\n",
      "Decoder target shape: (30, 14)\n",
      "\n",
      "======================================================================\n",
      "BUILDING SHALLOW AUTOENCODER-DECODER MODEL\n",
      "======================================================================\n",
      "Encoder built successfully\n",
      "Decoder built successfully\n",
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_14\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_14\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,040</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │           <span style=\"color: #00af00; text-decoration-color: #00af00\">7,808</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ encoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ not_equal_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)           │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">394,240</span> │ decoder_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]        │                 │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],        │\n",
       "│                               │                           │                 │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,677</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_input (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │           \u001b[38;5;34m7,040\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ not_equal_2 (\u001b[38;5;33mNotEqual\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_embedding (\u001b[38;5;33mEmbedding\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │           \u001b[38;5;34m7,808\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │         \u001b[38;5;34m394,240\u001b[0m │ encoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ not_equal_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)           │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,  │         \u001b[38;5;34m394,240\u001b[0m │ decoder_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]        │                 │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],        │\n",
       "│                               │                           │                 │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ decoder_dense (\u001b[38;5;33mDense\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m61\u001b[0m)            │          \u001b[38;5;34m15,677\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">819,005</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m819,005\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">819,005</span> (3.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m819,005\u001b[0m (3.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING THE MODEL\n",
      "======================================================================\n",
      "Training samples: 24\n",
      "Testing samples: 6\n",
      "Epoch 1/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.5506 - loss: 4.0842 - val_accuracy: 0.6786 - val_loss: 4.0386\n",
      "Epoch 2/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6994 - loss: 3.9135 - val_accuracy: 0.6786 - val_loss: 3.7100\n",
      "Epoch 3/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.6964 - loss: 3.2362 - val_accuracy: 0.6786 - val_loss: 3.0915\n",
      "Epoch 4/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6577 - loss: 3.0112 - val_accuracy: 0.7500 - val_loss: 3.0406\n",
      "Epoch 5/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7589 - loss: 2.8012 - val_accuracy: 0.7500 - val_loss: 3.0475\n",
      "Epoch 6/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.7619 - loss: 2.6949 - val_accuracy: 0.7500 - val_loss: 3.0473\n",
      "Epoch 7/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.7560 - loss: 2.5792 - val_accuracy: 0.7381 - val_loss: 3.0684\n",
      "Epoch 8/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7560 - loss: 2.4923 - val_accuracy: 0.7500 - val_loss: 3.0399\n",
      "Epoch 9/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7768 - loss: 2.4005 - val_accuracy: 0.7500 - val_loss: 2.9921\n",
      "Epoch 10/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7827 - loss: 2.3288 - val_accuracy: 0.7500 - val_loss: 3.0897\n",
      "Epoch 11/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.7857 - loss: 2.2510 - val_accuracy: 0.7619 - val_loss: 3.1030\n",
      "Epoch 12/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8065 - loss: 2.1794 - val_accuracy: 0.7738 - val_loss: 3.1330\n",
      "Epoch 13/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.8065 - loss: 2.0744 - val_accuracy: 0.7976 - val_loss: 3.2661\n",
      "Epoch 14/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.8393 - loss: 1.9671 - val_accuracy: 0.7976 - val_loss: 3.4515\n",
      "Epoch 15/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8452 - loss: 1.8476 - val_accuracy: 0.7619 - val_loss: 3.5462\n",
      "Epoch 16/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.6280 - loss: 1.7637 - val_accuracy: 0.4881 - val_loss: 3.6116\n",
      "Epoch 17/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5714 - loss: 1.6796 - val_accuracy: 0.2976 - val_loss: 3.7624\n",
      "Epoch 18/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2827 - loss: 1.7542 - val_accuracy: 0.2738 - val_loss: 4.1159\n",
      "Epoch 19/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4137 - loss: 1.5021 - val_accuracy: 0.3095 - val_loss: 3.6092\n",
      "Epoch 20/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2917 - loss: 1.4083 - val_accuracy: 0.1905 - val_loss: 3.7078\n",
      "Epoch 21/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2381 - loss: 1.3199 - val_accuracy: 0.1905 - val_loss: 3.8996\n",
      "Epoch 22/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2321 - loss: 1.1784 - val_accuracy: 0.1905 - val_loss: 3.9605\n",
      "Epoch 23/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.2530 - loss: 1.0875 - val_accuracy: 0.1905 - val_loss: 3.9484\n",
      "Epoch 24/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2619 - loss: 0.9834 - val_accuracy: 0.1905 - val_loss: 4.0468\n",
      "Epoch 25/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.2708 - loss: 0.9056 - val_accuracy: 0.1905 - val_loss: 4.1235\n",
      "Epoch 26/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.2857 - loss: 0.8100 - val_accuracy: 0.1905 - val_loss: 4.2377\n",
      "Epoch 27/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2917 - loss: 0.7367 - val_accuracy: 0.1905 - val_loss: 4.3066\n",
      "Epoch 28/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3006 - loss: 0.6637 - val_accuracy: 0.1905 - val_loss: 4.3737\n",
      "Epoch 29/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3006 - loss: 0.5894 - val_accuracy: 0.1786 - val_loss: 4.5156\n",
      "Epoch 30/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3274 - loss: 0.5149 - val_accuracy: 0.1786 - val_loss: 4.5646\n",
      "Epoch 31/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3333 - loss: 0.4623 - val_accuracy: 0.1905 - val_loss: 4.6422\n",
      "Epoch 32/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3244 - loss: 0.4399 - val_accuracy: 0.1786 - val_loss: 4.7088\n",
      "Epoch 33/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3423 - loss: 0.3884 - val_accuracy: 0.1905 - val_loss: 4.7354\n",
      "Epoch 34/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3482 - loss: 0.3595 - val_accuracy: 0.1548 - val_loss: 4.8233\n",
      "Epoch 35/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3512 - loss: 0.3273 - val_accuracy: 0.1786 - val_loss: 4.9044\n",
      "Epoch 36/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3512 - loss: 0.2910 - val_accuracy: 0.1786 - val_loss: 4.8412\n",
      "Epoch 37/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3631 - loss: 0.2637 - val_accuracy: 0.1905 - val_loss: 4.9549\n",
      "Epoch 38/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3631 - loss: 0.2413 - val_accuracy: 0.1786 - val_loss: 4.9069\n",
      "Epoch 39/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3661 - loss: 0.2102 - val_accuracy: 0.1905 - val_loss: 4.9443\n",
      "Epoch 40/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3661 - loss: 0.1786 - val_accuracy: 0.1905 - val_loss: 5.0374\n",
      "Epoch 41/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3631 - loss: 0.1782 - val_accuracy: 0.1786 - val_loss: 5.0702\n",
      "Epoch 42/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3601 - loss: 0.1629 - val_accuracy: 0.1905 - val_loss: 5.0358\n",
      "Epoch 43/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3631 - loss: 0.1483 - val_accuracy: 0.1786 - val_loss: 5.1345\n",
      "Epoch 44/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3661 - loss: 0.1332 - val_accuracy: 0.1905 - val_loss: 5.1924\n",
      "Epoch 45/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3720 - loss: 0.1195 - val_accuracy: 0.1905 - val_loss: 5.1721\n",
      "Epoch 46/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3720 - loss: 0.1036 - val_accuracy: 0.1786 - val_loss: 5.2015\n",
      "Epoch 47/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3720 - loss: 0.0954 - val_accuracy: 0.1786 - val_loss: 5.2639\n",
      "Epoch 48/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3720 - loss: 0.0900 - val_accuracy: 0.1786 - val_loss: 5.2828\n",
      "Epoch 49/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0817 - val_accuracy: 0.1786 - val_loss: 5.3090\n",
      "Epoch 50/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.0716 - val_accuracy: 0.1905 - val_loss: 5.3221\n",
      "Epoch 51/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0687 - val_accuracy: 0.1905 - val_loss: 5.3564\n",
      "Epoch 52/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3750 - loss: 0.0635 - val_accuracy: 0.1786 - val_loss: 5.4169\n",
      "Epoch 53/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0554 - val_accuracy: 0.1786 - val_loss: 5.4107\n",
      "Epoch 54/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3750 - loss: 0.0522 - val_accuracy: 0.1786 - val_loss: 5.4050\n",
      "Epoch 55/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 0.0475 - val_accuracy: 0.1905 - val_loss: 5.4467\n",
      "Epoch 56/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 0.0435 - val_accuracy: 0.1905 - val_loss: 5.4743\n",
      "Epoch 57/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0400 - val_accuracy: 0.1905 - val_loss: 5.4999\n",
      "Epoch 58/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0377 - val_accuracy: 0.1905 - val_loss: 5.5164\n",
      "Epoch 59/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 0.0343 - val_accuracy: 0.1905 - val_loss: 5.5204\n",
      "Epoch 60/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3750 - loss: 0.0324 - val_accuracy: 0.1905 - val_loss: 5.5374\n",
      "Epoch 61/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3750 - loss: 0.0306 - val_accuracy: 0.1905 - val_loss: 5.5579\n",
      "Epoch 62/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3750 - loss: 0.0281 - val_accuracy: 0.1905 - val_loss: 5.5711\n",
      "Epoch 63/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.0266 - val_accuracy: 0.1905 - val_loss: 5.5813\n",
      "Epoch 64/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3750 - loss: 0.0253 - val_accuracy: 0.1905 - val_loss: 5.5935\n",
      "Epoch 65/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3750 - loss: 0.0238 - val_accuracy: 0.1905 - val_loss: 5.6083\n",
      "Epoch 66/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0228 - val_accuracy: 0.1905 - val_loss: 5.6218\n",
      "Epoch 67/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0214 - val_accuracy: 0.1905 - val_loss: 5.6360\n",
      "Epoch 68/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3750 - loss: 0.0201 - val_accuracy: 0.1905 - val_loss: 5.6474\n",
      "Epoch 69/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0192 - val_accuracy: 0.1905 - val_loss: 5.6552\n",
      "Epoch 70/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.3750 - loss: 0.0184 - val_accuracy: 0.1905 - val_loss: 5.6640\n",
      "Epoch 71/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3750 - loss: 0.0175 - val_accuracy: 0.1905 - val_loss: 5.6754\n",
      "Epoch 72/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3750 - loss: 0.0168 - val_accuracy: 0.1905 - val_loss: 5.6866\n",
      "Epoch 73/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.3750 - loss: 0.0161 - val_accuracy: 0.1905 - val_loss: 5.6941\n",
      "Epoch 74/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3750 - loss: 0.0152 - val_accuracy: 0.1905 - val_loss: 5.7033\n",
      "Epoch 75/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0148 - val_accuracy: 0.1905 - val_loss: 5.7116\n",
      "Epoch 76/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0140 - val_accuracy: 0.1905 - val_loss: 5.7216\n",
      "Epoch 77/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3750 - loss: 0.0138 - val_accuracy: 0.1905 - val_loss: 5.7318\n",
      "Epoch 78/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3750 - loss: 0.0131 - val_accuracy: 0.1905 - val_loss: 5.7372\n",
      "Epoch 79/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3750 - loss: 0.0125 - val_accuracy: 0.1905 - val_loss: 5.7412\n",
      "Epoch 80/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0121 - val_accuracy: 0.1905 - val_loss: 5.7492\n",
      "Epoch 81/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3750 - loss: 0.0119 - val_accuracy: 0.1905 - val_loss: 5.7606\n",
      "Epoch 82/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3750 - loss: 0.0114 - val_accuracy: 0.1905 - val_loss: 5.7693\n",
      "Epoch 83/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3750 - loss: 0.0110 - val_accuracy: 0.1905 - val_loss: 5.7768\n",
      "Epoch 84/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0106 - val_accuracy: 0.1905 - val_loss: 5.7823\n",
      "Epoch 85/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0105 - val_accuracy: 0.1905 - val_loss: 5.7883\n",
      "Epoch 86/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.0102 - val_accuracy: 0.1905 - val_loss: 5.7942\n",
      "Epoch 87/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3750 - loss: 0.0096 - val_accuracy: 0.1905 - val_loss: 5.8019\n",
      "Epoch 88/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0095 - val_accuracy: 0.1905 - val_loss: 5.8072\n",
      "Epoch 89/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3750 - loss: 0.0092 - val_accuracy: 0.1905 - val_loss: 5.8137\n",
      "Epoch 90/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0089 - val_accuracy: 0.1905 - val_loss: 5.8218\n",
      "Epoch 91/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0086 - val_accuracy: 0.1905 - val_loss: 5.8271\n",
      "Epoch 92/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.3750 - loss: 0.0085 - val_accuracy: 0.1905 - val_loss: 5.8312\n",
      "Epoch 93/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0082 - val_accuracy: 0.1905 - val_loss: 5.8377\n",
      "Epoch 94/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0080 - val_accuracy: 0.1905 - val_loss: 5.8436\n",
      "Epoch 95/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3750 - loss: 0.0079 - val_accuracy: 0.1905 - val_loss: 5.8484\n",
      "Epoch 96/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.0077 - val_accuracy: 0.1905 - val_loss: 5.8541\n",
      "Epoch 97/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0074 - val_accuracy: 0.1905 - val_loss: 5.8609\n",
      "Epoch 98/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.3750 - loss: 0.0073 - val_accuracy: 0.1905 - val_loss: 5.8682\n",
      "Epoch 99/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3750 - loss: 0.0070 - val_accuracy: 0.1905 - val_loss: 5.8737\n",
      "Epoch 100/100\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3750 - loss: 0.0069 - val_accuracy: 0.1905 - val_loss: 5.8771\n",
      "\n",
      "Training completed!\n",
      "Final training loss: 0.0069\n",
      "Final training accuracy: 0.3750\n",
      "Final validation loss: 5.8771\n",
      "Final validation accuracy: 0.1905\n",
      "\n",
      "======================================================================\n",
      "BUILDING INFERENCE MODELS\n",
      "======================================================================\n",
      "Inference models created successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# ============================================================================\n",
    "# 1. LOAD AND PREPARE DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LOADING KAGGLE ENGLISH-HINDI TRANSLATION DATASET\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# For demonstration, we'll create a sample dataset\n",
    "# To use the actual Kaggle dataset, download it and load using:\n",
    "# df = pd.read_csv('Hindi_English_Truncated_Corpus.csv')\n",
    "\n",
    "# Sample English-Hindi pairs (replace with actual Kaggle dataset)\n",
    "data = {\n",
    "    'english_sentence': [\n",
    "        'hello', 'how are you', 'good morning', 'thank you', 'goodbye',\n",
    "        'what is your name', 'where are you from', 'i am fine', \n",
    "        'nice to meet you', 'have a nice day', 'how old are you',\n",
    "        'what do you do', 'i love learning', 'this is great',\n",
    "        'see you later', 'please help me', 'i am happy', 'welcome',\n",
    "        'good night', 'where is the market', 'i am hungry',\n",
    "        'what time is it', 'i need water', 'how much does it cost',\n",
    "        'i like this', 'can you help', 'i am learning hindi',\n",
    "        'what is this', 'where do you live', 'i want to go'\n",
    "    ],\n",
    "    'hindi_sentence': [\n",
    "        'नमस्ते', 'आप कैसे हैं', 'सुप्रभात', 'धन्यवाद', 'अलविदा',\n",
    "        'आपका नाम क्या है', 'आप कहां से हैं', 'मैं ठीक हूं',\n",
    "        'आपसे मिलकर अच्छा लगा', 'आपका दिन शुभ हो', 'आपकी उम्र क्या है',\n",
    "        'आप क्या करते हैं', 'मुझे सीखना पसंद है', 'यह बहुत अच्छा है',\n",
    "        'फिर मिलेंगे', 'कृपया मेरी मदद करें', 'मैं खुश हूं', 'स्वागत है',\n",
    "        'शुभ रात्रि', 'बाजार कहां है', 'मुझे भूख लगी है',\n",
    "        'समय क्या हुआ है', 'मुझे पानी चाहिए', 'इसकी कीमत क्या है',\n",
    "        'मुझे यह पसंद है', 'क्या आप मदद कर सकते हैं', 'मैं हिंदी सीख रहा हूं',\n",
    "        'यह क्या है', 'आप कहां रहते हैं', 'मैं जाना चाहता हूं'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"Dataset loaded with {len(df)} sentence pairs\")\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPROCESSING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Clean and preprocess sentences\"\"\"\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Add space between punctuation\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# Preprocess English sentences\n",
    "df['english_sentence'] = df['english_sentence'].apply(preprocess_sentence)\n",
    "\n",
    "# Add start and end tokens to Hindi sentences\n",
    "df['hindi_sentence'] = df['hindi_sentence'].apply(lambda x: '<start> ' + x + ' <end>')\n",
    "\n",
    "english_sentences = df['english_sentence'].tolist()\n",
    "hindi_sentences = df['hindi_sentence'].tolist()\n",
    "\n",
    "print(f\"Total sentences: {len(english_sentences)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. TOKENIZATION AND VOCABULARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOKENIZATION AND VOCABULARY BUILDING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create tokenizers\n",
    "eng_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "eng_tokenizer.fit_on_texts(english_sentences)\n",
    "\n",
    "hin_tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "hin_tokenizer.fit_on_texts(hindi_sentences)\n",
    "\n",
    "# Vocabulary sizes\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
    "\n",
    "print(f\"English vocabulary size: {eng_vocab_size}\")\n",
    "print(f\"Hindi vocabulary size: {hin_vocab_size}\")\n",
    "\n",
    "# Convert to sequences\n",
    "eng_sequences = eng_tokenizer.texts_to_sequences(english_sentences)\n",
    "hin_sequences = hin_tokenizer.texts_to_sequences(hindi_sentences)\n",
    "\n",
    "# Pad sequences\n",
    "max_eng_len = 15\n",
    "max_hin_len = 15\n",
    "\n",
    "eng_padded = pad_sequences(eng_sequences, maxlen=max_eng_len, padding='post')\n",
    "hin_padded = pad_sequences(hin_sequences, maxlen=max_hin_len, padding='post')\n",
    "\n",
    "print(f\"English padded shape: {eng_padded.shape}\")\n",
    "print(f\"Hindi padded shape: {hin_padded.shape}\")\n",
    "\n",
    "# Prepare decoder input and target\n",
    "decoder_input = hin_padded[:, :-1]\n",
    "decoder_target = hin_padded[:, 1:]\n",
    "\n",
    "print(f\"Decoder input shape: {decoder_input.shape}\")\n",
    "print(f\"Decoder target shape: {decoder_target.shape}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUILD SHALLOW AUTOENCODER MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING SHALLOW AUTOENCODER-DECODER MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 128\n",
    "latent_dim = 256\n",
    "\n",
    "# ENCODER\n",
    "encoder_inputs = layers.Input(shape=(max_eng_len,), name='encoder_input')\n",
    "encoder_embedding = layers.Embedding(\n",
    "    eng_vocab_size, \n",
    "    embedding_dim, \n",
    "    mask_zero=True,\n",
    "    name='encoder_embedding'\n",
    ")(encoder_inputs)\n",
    "\n",
    "# Shallow encoder - single LSTM layer\n",
    "encoder_lstm = layers.LSTM(\n",
    "    latent_dim, \n",
    "    return_state=True,\n",
    "    name='encoder_lstm'\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "print(\"Encoder built successfully\")\n",
    "\n",
    "# DECODER\n",
    "decoder_inputs = layers.Input(shape=(max_hin_len - 1,), name='decoder_input')\n",
    "decoder_embedding = layers.Embedding(\n",
    "    hin_vocab_size, \n",
    "    embedding_dim, \n",
    "    mask_zero=True,\n",
    "    name='decoder_embedding'\n",
    ")(decoder_inputs)\n",
    "\n",
    "# Shallow decoder - single LSTM layer\n",
    "decoder_lstm = layers.LSTM(\n",
    "    latent_dim, \n",
    "    return_sequences=True, \n",
    "    return_state=True,\n",
    "    name='decoder_lstm'\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "    decoder_embedding, \n",
    "    initial_state=encoder_states\n",
    ")\n",
    "\n",
    "# Output layer\n",
    "decoder_dense = layers.Dense(\n",
    "    hin_vocab_size, \n",
    "    activation='softmax',\n",
    "    name='decoder_dense'\n",
    ")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "print(\"Decoder built successfully\")\n",
    "\n",
    "# Complete model\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. TRAIN THE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING THE MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Split data\n",
    "X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(\n",
    "    eng_padded, decoder_input, decoder_target, \n",
    "    test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train_enc)}\")\n",
    "print(f\"Testing samples: {len(X_test_enc)}\")\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    [X_train_enc, X_train_dec],\n",
    "    np.expand_dims(y_train, -1),\n",
    "    batch_size=4,\n",
    "    epochs=100,\n",
    "    validation_data=([X_test_enc, X_test_dec], np.expand_dims(y_test, -1)),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. BUILD INFERENCE MODELS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BUILDING INFERENCE MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = layers.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_embedding_inf = model.get_layer('decoder_embedding')\n",
    "decoder_lstm_inf = model.get_layer('decoder_lstm')\n",
    "decoder_dense_inf = model.get_layer('decoder_dense')\n",
    "\n",
    "decoder_inputs_single = layers.Input(shape=(1,))\n",
    "decoder_embedding_output = decoder_embedding_inf(decoder_inputs_single)\n",
    "decoder_outputs, state_h, state_c = decoder_lstm_inf(\n",
    "    decoder_embedding_output, \n",
    "    initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense_inf(decoder_outputs)\n",
    "\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "print(\"Inference models created successfully\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. TRANSLATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def translate_sentence(input_sentence):\n",
    "    \"\"\"Translate English sentence to Hindi\"\"\"\n",
    "    # Preprocess input\n",
    "    input_sentence = preprocess_sentence(input_sentence)\n",
    "    \n",
    "    # Encode input\n",
    "    input_seq = eng_tokenizer.texts_to_sequences([input_sentence])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_eng_len, padding='post')\n",
    "    \n",
    "    # Get encoder states\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    # Generate empty target sequence\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = hin_tokenizer.word_index.get('<start>', 1)\n",
    "    \n",
    "    # Decode\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, \n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Sample token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        # Get word\n",
    "        sampled_word = None\n",
    "        for word, index in hin_tokenizer.word_index.items():\n",
    "            if sampled_token_index == index:\n",
    "                sampled_word = word\n",
    "                break\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_sentence) > max_hin_len:\n",
    "            stop_condition = True\n",
    "        elif sampled_word and sampled_word != '<start>':\n",
    "            decoded_sentence.append(sampled_word)\n",
    "        \n",
    "        # Update target sequence\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a06350e-5d8e-4c63-a418-64c7f9d0d2fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "INTERACTIVE TRANSLATION MODE\n",
      "======================================================================\n",
      "Enter English sentences to translate (type 'quit' to exit)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "English:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi:   नमस्ते\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "English:  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi:   आप कैसे हैं\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "English:  thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi:   धन्यवाद\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "English:  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation session ended.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. INTERACTIVE TRANSLATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"INTERACTIVE TRANSLATION MODE\")\n",
    "print(\"=\"*70)\n",
    "print(\"Enter English sentences to translate (type 'quit' to exit)\")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nEnglish: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    \n",
    "    try:\n",
    "        translation = translate_sentence(user_input)\n",
    "        print(f\"Hindi:   {translation}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"\\nTranslation session ended.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8797566-df10-4cbf-96ff-cdb7a069645b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
